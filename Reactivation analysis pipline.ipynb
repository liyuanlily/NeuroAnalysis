{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing some modules \n",
    "\n",
    "import os \n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "from heapq import nsmallest\n",
    "from scipy.signal import chirp, find_peaks, peak_widths\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from os import walk\n",
    "from ast import literal_eval\n",
    "import re \n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter\n",
    "# import more_itertools as mit\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# start_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "# Date = date.today.strftime(\"%d%m%Y\")\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import zscore, bootstrap, sem\n",
    "from scipy.signal import savgol_filter as sg_filter\n",
    "from scipy.signal import gaussian\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import wilcoxon, pearsonr\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "# from sklearn.prepocessing import normalize\n",
    "\n",
    "import copy\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.weightstats import ztest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b86661",
   "metadata": {},
   "source": [
    "### Edgar's code of prepocessing data and assembly analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb66c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERAL FUNCTIONS \n",
    "def generate_folder_path(dirs):\n",
    "    for i in dirs:\n",
    "        try:\n",
    "            os.makedirs(i)\n",
    "            print(\"Directory \" , i ,  \" Created \")\n",
    "        except FileExistsError:\n",
    "            print(\"Directory \" , i ,  \" already exists\")  \n",
    "            \n",
    "def load_stim_file(dp_StimF): \n",
    "    os.chdir(dp_StimF)\n",
    "    StimFs = []\n",
    "    StimFs.append(genfromtxt('StimF1.csv', delimiter=',').astype(int))\n",
    "    return StimFs[0]\n",
    "\n",
    "\n",
    "def load_s2p_files(dp_s2p, neuropil_correction):\n",
    "    \"\"\" this function loads the suite2p output files \n",
    "    #can refine this to load only a subset of files \n",
    "    https://mouseland.github.io/suite2p/_build/html/outputs.html \n",
    "    Parameters: s2p datapath, neuropil correction value (if 0, do no correction) \n",
    "    Returns:F, Spks, ops,iscell,stat,Fneu \"\"\" \n",
    "    \n",
    "    os.chdir(dp_s2p)\n",
    "    F = np.load('F.npy')\n",
    "    Spks = np.load('Spks.npy')\n",
    "    ops = np.load('ops.npy', allow_pickle=True).item() #from the suite2p github \n",
    "    iscell = np.load('iscell.npy') #first col is binary yes or no and second col is prob. classifier that is cell \n",
    "    stat = np.load('stat.npy', allow_pickle=True)\n",
    "    Fneu = np.load('Fneu.npy')\n",
    "\n",
    "    if neuropil_correction > 0: \n",
    "        F = F-Fneu*neuropil_correction\n",
    "        for idx,i in enumerate(F): \n",
    "            F[idx] = i-np.min(i)\n",
    "    return F, Spks, ops,iscell,stat,Fneu\n",
    "\n",
    "def pre_process_imaging(iscell, F, stat, FOVsizeum, mode):\n",
    "    \"\"\" 'Gives you dff for cells of interest and stim triggers \n",
    "    Paramters: iscell, F, stat, FOVsizeum, dp_StimF, mode (median or 10% median dff)\n",
    "    Returns: FNc, iscell_list, x,y, StimFs\n",
    "    \n",
    "    '\"\"\"\n",
    "    iscell_list = get_curated_cells(iscell)\n",
    "    Fc = F[iscell_list] \n",
    "    if mode == 'median':\n",
    "        FNc = dff_median(Fc)\n",
    "    elif mode == '10': \n",
    "        FNc = dff_10percent(Fc)\n",
    "    x, y = get_cell_centroids(stat, iscell_list)\n",
    "    xa = [i*(FOVsizeum/512) for i in x]\n",
    "    yb = [i*(FOVsizeum/512) for i in y]\n",
    "    return FNc, iscell_list, xa,yb, x,y\n",
    "\n",
    "def get_cell_centroids(stat, index_list):\n",
    "    #this function finds the x and y centroids from the stat file from suite2p \n",
    "    x,y = zip(*[(stat[i]['med'][1], stat[i]['med'][0]) for i in index_list])\n",
    "    return x,y\n",
    "\n",
    "def get_curated_cells(iscell): \n",
    "    return np.where(iscell[:,0] == 1)[0]\n",
    "\n",
    "def dff_10percent(traces): \n",
    "    a = np.empty_like(traces) \n",
    "    k = int(len(traces[0])/10)\n",
    "    for idx,i in enumerate(traces): \n",
    "        bsl = np.median(i[np.argpartition(i, k)[:k]])\n",
    "        a[idx] = (i-bsl)/bsl\n",
    "    return a\n",
    "def dff_median(traces): \n",
    "    a = np.empty_like(traces) \n",
    "    k = int(len(traces[0])/10)\n",
    "    for idx,i in enumerate(traces): \n",
    "        bsl = np.median(i)#[np.argpartition(i, k)[:k]])\n",
    "        a[idx] = (i-bsl)/bsl\n",
    "    return a\n",
    "\n",
    "def grab_file_info(dp): \n",
    "    \"\"\" This function gets the relevant information of a file from the name \n",
    "        \n",
    "        Parameters: datapath \n",
    "\n",
    "        Returns: Mouse_ID, Date\n",
    "\n",
    "    \"\"\" \n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(dp):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    Mouse_ID = f[0].split('.')[0][6:9]\n",
    "    Date = f[0].split('.')[0][:6]\n",
    "    return Mouse_ID, Date\n",
    "def sorted_nicely( l ): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "def behaviour_trial_function_1Map(data, start_trial):\n",
    "    \n",
    "    \n",
    "    behav_trial_lengths = [len(i) for i in data[start_trial:]]\n",
    "    behav_trials = []\n",
    "    for i in data[start_trial:]: \n",
    "        this_trial = i\n",
    "        this_trial_list = []\n",
    "        for u in this_trial: \n",
    "            this_trial_list.append(u[0])\n",
    "        behav_trials.append(this_trial_list)\n",
    "        \n",
    "    return behav_trial_lengths, behav_trials\n",
    "\n",
    "def get_imaging_trial_lengths(Triggers):\n",
    "    \n",
    "    img_trial_lengths = []\n",
    "    for idx, i in enumerate(Triggers[:-1]):\n",
    "        img_trial_lengths.append(Triggers[idx+1] - Triggers[idx])\n",
    "        \n",
    "    return img_trial_lengths\n",
    "\n",
    "def re_sample_behaviour(behav_trials, img_trial_lengths, y_start, y_end): \n",
    "    re_sampled_behav = []\n",
    "    for idx, i in enumerate(behav_trials): \n",
    "        re_sample = scipy.signal.resample(i, img_trial_lengths[idx])\n",
    "        re_sample[re_sample < y_start] = y_start\n",
    "        re_sample[re_sample > y_end] = y_end\n",
    "        re_sample[0:10], re_sample[-10:] = y_start, y_end\n",
    "        re_sampled_behav.append(re_sample)\n",
    "    return re_sampled_behav\n",
    "\n",
    "def re_sample_speed(behav_trials, img_trial_lengths): \n",
    "    re_sampled_behav = []\n",
    "    for idx, i in enumerate(behav_trials): \n",
    "        re_sample = scipy.signal.resample(i, img_trial_lengths[idx])\n",
    "#         re_sample[re_sample < y_start] = y_start\n",
    "#         re_sample[re_sample > y_end] = y_end\n",
    "#         re_sample[0:10], re_sample[-10:] = y_start, y_end\n",
    "        re_sampled_behav.append(re_sample)\n",
    "    return re_sampled_behav\n",
    "\n",
    "def normalizedata(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# @jit(nopython=True)\n",
    "def interpolation(arr_3d,):\n",
    "    result=np.zeros_like(arr_3d)\n",
    "    for i in range(arr_3d.shape[0]):\n",
    "        for j in range(arr_3d.shape[1]):\n",
    "            arr=arr_3d[i,j,:]\n",
    "            # If all elements are nan then cannot conduct linear interpolation.\n",
    "            if np.sum(np.isnan(arr))==arr.shape[0]:\n",
    "                result[i,j,:]=arr\n",
    "            else:\n",
    "                # If the first elemet is nan, then assign the value of its right nearest neighbor to it.\n",
    "                if np.isnan(arr[0]):\n",
    "                    arr[0]=arr[~np.isnan(arr)][0]\n",
    "                # If the last element is nan, then assign the value of its left nearest neighbor to it.\n",
    "                if np.isnan(arr[-1]):\n",
    "                    arr[-1]=arr[~np.isnan(arr)][-1]\n",
    "                # If the element is in the middle and its value is nan, do linear interpolation using neighbor values.\n",
    "                for k in range(arr.shape[0]):\n",
    "                    if np.isnan(arr[k]):\n",
    "                        x=k\n",
    "                        x1=x-1\n",
    "                        x2=x+1\n",
    "                        # Find left neighbor whose value is not nan.\n",
    "                        while x1>=0:\n",
    "                            if np.isnan(arr[x1]):\n",
    "                                x1=x1-1\n",
    "                            else:\n",
    "                                y1=arr[x1]\n",
    "                                break\n",
    "                        # Find right neighbor whose value is not nan.\n",
    "                        while x2<arr.shape[0]:\n",
    "                            if np.isnan(arr[x2]):\n",
    "                                x2=x2+1\n",
    "                            else:\n",
    "                                y2=arr[x2]\n",
    "                                break\n",
    "                        # Calculate the slope and intercept determined by the left and right neighbors.\n",
    "                        slope=(y2-y1)/(x2-x1)\n",
    "                        intercept=y1-slope*x1\n",
    "                        # Linear interpolation and assignment.\n",
    "                        y=slope*x+intercept\n",
    "                        arr[x]=y\n",
    "                result[i,j,:]=arr\n",
    "    return result\n",
    "\n",
    "def behaviour_pre_processing(dp_behav, StimFs, y_start, y_end): \n",
    "    os.chdir(dp_behav)\n",
    "    Mouse_ID, Date = grab_file_info(dp_behav)\n",
    "    behaviour_data = fully_sorted_data(dp_behav, Date)    \n",
    "    behav_trial_lengths, behav_trials = behaviour_trial_function_1Map(behaviour_data, 1)\n",
    "    img_trial_lengths = get_imaging_trial_lengths(StimFs)\n",
    "    re_sampled_behav = re_sample_behaviour(behav_trials, img_trial_lengths, y_start, y_end)\n",
    "\n",
    "    behav_speed = []\n",
    "    for t in behav_trials:\n",
    "        speed_list = []\n",
    "        for i in range(20,len(t)):\n",
    "            speed_list.append((np.max(t[i-20:i])-np.min(t[i-20:i]))*0.5/(20/55))\n",
    "        speed_list = [speed_list[0]]*20 + speed_list\n",
    "        behav_speed.append(speed_list)\n",
    "    re_sampled_speed = re_sample_speed(behav_speed, img_trial_lengths)\n",
    "\n",
    "\n",
    "    return re_sampled_behav, re_sampled_speed, behav_trial_lengths\n",
    "\n",
    "def fully_sorted_data(dp_behav, Date):\n",
    "    os.chdir(dp_behav)\n",
    "    file_names = [i for i in os.listdir(dp_behav) if os.path.isfile(os.path.join(dp_behav,i)) and Date in i[:6]]\n",
    "    file_names2 = []\n",
    "    for x in sorted_nicely(file_names):\n",
    "        file_names2.append(x)\n",
    "    data  = []\n",
    "    for i in file_names2:\n",
    "        with open(i, 'r') as f:\n",
    "            this_data = literal_eval('[' + ''.join(f.readlines()) + ']')\n",
    "        data.append(this_data[0])\n",
    "    return data \n",
    "\n",
    "\n",
    "#PLACE CELL FUNCTIONS \n",
    "\n",
    "def new_rate_map(traces, speed, y_start, y_end, binsize, triggers, trial_n, re_sampled_behav):\n",
    "    tracesc = np.array(traces, copy=True)  \n",
    "    bins = np.arange(y_start, y_end+binsize, binsize)\n",
    "    master = np.zeros([len(trial_n), len(tracesc), len(bins)] )\n",
    "    for tdx, t in enumerate(trial_n): \n",
    "        bt = re_sampled_behav[t]\n",
    "        st = speed[t]\n",
    "        tt = tracesc[:,triggers[t]:triggers[t+1]]\n",
    "        dig = np.digitize(bt,bins)\n",
    "        tt[:, np.where(st < 5)[0]] = np.nan\n",
    "        master[tdx] = np.array([tt[:,dig == i].mean(axis=1) for i in range(0,len(bins))]).transpose()\n",
    "    return interpolation(master)\n",
    "\n",
    "def new_rate_map_shuffle(traces, speed, y_start, y_end, binsize, triggers, trial_n, re_sampled_behav):\n",
    "    tracesc = np.array(traces, copy=True)  \n",
    "    bins = np.arange(y_start, y_end+binsize, binsize)\n",
    "    master = np.zeros([len(trial_n), len(tracesc), len(bins)] )\n",
    "    for tdx, t in enumerate(trial_n): \n",
    "        bt = re_sampled_behav[t]\n",
    "        st = speed[t]\n",
    "        tt = np.roll(tracesc[:,triggers[t]:triggers[t+1]], random.randint(5*30, 100000))\n",
    "        dig = np.digitize(bt,bins)\n",
    "        tt[:, np.where(st < 5)[0]] = np.nan\n",
    "        master[tdx] = np.array([tt[:,dig == i].mean(axis=1) for i in range(0,len(bins))]).transpose()\n",
    "    return interpolation(master)\n",
    "\n",
    "def peaksort(alist):\n",
    "    return np.argmax(alist)\n",
    "\n",
    "def place_cell_dombeck(trace):\n",
    "    #trace = gaussian_filter(Map1_mean_traces[221],sigma = 3)\n",
    "    max_bins = 80\n",
    "    min_bins = 4\n",
    "    df_thresh = 0.1\n",
    "\n",
    "#     trace = trace-np.min(trace)\n",
    "    peak = np.max(trace)\n",
    "    minn = np.median(sorted(trace)[:int(len(trace)/4)])\n",
    "    greater_indicies = [udx for udx, u in enumerate(trace) if u > (peak-minn)/4]\n",
    "    fields = [list(group) for group in mit.consecutive_groups(greater_indicies)]\n",
    "    big_enough_fields = [i for i in fields if max_bins >len(i) > min_bins]\n",
    "    percent_20 = []\n",
    "    for i in big_enough_fields: \n",
    "        if len([c for c in [trace[u] for u in i] if c > df_thresh]) > 1: \n",
    "            percent_20.append(i)\n",
    "    thresholded_field = []\n",
    "    for i in percent_20: \n",
    "        if np.mean([trace[u] for u in i]) > np.mean([c for cdx, c in enumerate(trace) if cdx not in i])*3: \n",
    "            thresholded_field.append(i)\n",
    "    return thresholded_field\n",
    "\n",
    "def calculate_map_stability(rate_map): \n",
    "    corr = []\n",
    "    for idx, i in enumerate(rate_map): \n",
    "        remaing = rate_map[(np.arange(rate_map.shape[0]) != idx),:]\n",
    "        corr.append(np.corrcoef(i, np.mean(remaing,axis=0))[1,0])\n",
    "    return np.mean(corr)\n",
    "\n",
    "def my_z_score(dist, value):\n",
    "    mean = np.mean(dist)\n",
    "    std = np.std(dist)\n",
    "    return (value-mean)/std\n",
    "\n",
    "def my_place_cell_func(smoothed_trace, shuffled_peaks): \n",
    "    return my_z_score(shuffled_peaks,np.max(smoothed_trace))\n",
    "\n",
    "\n",
    "def calculate_map_stability(rate_map, Sigma): \n",
    "    corr = []\n",
    "    for idx, i in enumerate(rate_map): \n",
    "        remaing = rate_map[(np.arange(rate_map.shape[0]) != idx),:]\n",
    "        corr.append(np.corrcoef(gaussian_filter(i,sigma = Sigma), gaussian_filter(np.mean(remaing,axis=0),sigma = Sigma))[1,0])\n",
    "    return corr\n",
    "\n",
    "def FWHM(X,Y):\n",
    "    half_max = max(Y) / 2\n",
    "    #find when function crosses line half_max (when sign of diff flips)\n",
    "    #take the 'derivative' of signum(half_max - Y[])\n",
    "    d = np.sign(half_max - np.array(Y[0:-1])) - np.sign(half_max - np.array(Y[1:]))\n",
    "    #plot(X[0:len(d)],d) #if you are interested\n",
    "    #find the left and right most indexes\n",
    "    left_idx = np.where(d > 0)[0]\n",
    "    right_idx = np.where(d < 0)[-1]\n",
    "    return left_idx,right_idx # X[right_idx], X[left_idx] #return the difference (full width)\n",
    "\n",
    "def PC_FWHM_field_width(trace):\n",
    "    bsl = np.median(sorted(trace)[:int(len(trace)/5)]) #median of 20% lowest values\n",
    "    trace = trace - bsl\n",
    "    r_trace = trace[::-1]\n",
    "    try: \n",
    "        r = np.argmax(trace) + next(xdx for xdx, x in enumerate(trace[np.argmax(trace):]) if x < np.max(trace)/2)\n",
    "    except: r = 200\n",
    "    try:\n",
    "        l = np.argmax(trace) - next(xdx for xdx, x in enumerate(r_trace[np.argmax(r_trace):]) if x < np.max(trace)/2)\n",
    "    except: l = 0\n",
    "    return r-l\n",
    "\n",
    "# def new_rate_map(traces, speed, y_start, y_end, binsize, triggers, trial_n, re_sampled_behav):\n",
    "#     tracesc = np.array(traces, copy=True)  \n",
    "#     bins = np.arange(y_start, y_end+binsize, binsize)\n",
    "#     master = np.zeros([len(trial_n), len(tracesc), len(bins)] )\n",
    "#     for tdx, t in enumerate(trial_n): \n",
    "#         print(tdx)\n",
    "#         bt = re_sampled_behav[t]\n",
    "#         st = speed[t]\n",
    "#         tt = tracesc[:,triggers[t]:triggers[t+1]]\n",
    "#         dig = np.digitize(bt,bins)\n",
    "#         tt[:, np.where(st < 5)[0]] = np.nan\n",
    "#         master[tdx] = np.array([tt[:,dig == i].mean(axis=1) for i in range(0,len(bins))]).transpose()\n",
    "#     return interpolation(master)\n",
    "\n",
    "def position_and_speed_img_vec(FNc_behav, re_sampled_behav, re_sampled_speed, StimFs):\n",
    "    flat_list_a = [x for xs in re_sampled_behav for x in xs]\n",
    "    flat_list_b = [x for xs in re_sampled_speed for x in xs]\n",
    "\n",
    "    a = np.array(flat_list_a)\n",
    "    b = np.array(flat_list_b)\n",
    "    missed_start = np.empty(StimFs[0]) * np.nan\n",
    "    missed_end = np.empty(FNc_behav.shape[1]-StimFs[-1]) * np.nan\n",
    "    behav =  np.concatenate ([missed_start, a, missed_end])\n",
    "    speed = np.concatenate ([missed_start, b, missed_end])\n",
    "    return behav, speed\n",
    "\n",
    "\n",
    "# Defining some functions\n",
    "def mpPDF(var, q, pts):\n",
    "    \"\"\"\n",
    "    Creates a Marchenko-Pastur Probability Density Function\n",
    "\n",
    "    Input\n",
    "    ----------\n",
    "    var: Variance (float)\n",
    "    q: T/N where T is the number of rows and N the number of columns (n_neurons/n_samples)\n",
    "    pts: int for the number of points used to build PDF\n",
    "    Returns\n",
    "    -------\n",
    "    pandas series Marchenko-Pastur PDF\n",
    "\n",
    "    Source\n",
    "    ------\n",
    "    Adapted from https://medium.com/swlh/an-empirical-view-of-marchenko-pastur-theorem-1f564af5603d\n",
    "\n",
    "    Ref\n",
    "    ---\n",
    "    Marchenko and Pastur 1967 - https://www.researchgate.net/publication/303008084_Distribution_of_eigenvalues_for_some_sets_of_random_matrices\n",
    "    \"\"\"\n",
    "    # Marchenko-Pastur pdf\n",
    "    # q=T/N\n",
    "    # Adjusting code to work with 1 dimension arrays\n",
    "    if isinstance(var, np.ndarray):\n",
    "        if var.shape == (1,):\n",
    "            var = var[0]\n",
    "    eMin, eMax = var * (1 - (1. / q) ** .5) ** 2, var * (1 + (1. / q) ** .5) ** 2\n",
    "    eVal = np.linspace(eMin, eMax, pts)\n",
    "    pdf = q / (2 * np.pi * var * eVal) * ((eMax - eVal) * (eVal - eMin)) ** .5\n",
    "    pdf = pd.Series(pdf, index=eVal)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def fitKDE(obs, bWidth=.25, kernel='gaussian', x=None):\n",
    "    \"\"\"\n",
    "    Fit kernel to a series of obs, and derive the prob of obs x is the array of values\n",
    "        on which the fit KDE will be evaluated. It is the empirical PDF\n",
    "\n",
    "    Input\n",
    "    ----------\n",
    "    obs: array of observations to fit (eigenvalues)\n",
    "    bWidth: The bandwidth of the kernel (0.25 default)\n",
    "    kernel: string of which kerne to use ([‘gaussian’|’tophat’|’epanechnikov’|’exponential’|’linear’|’cosine’]), ('gaussian' default)\n",
    "    x: array of values on which fit KDE is evaluated\n",
    "    Returns\n",
    "    -------\n",
    "    pandas series Emperical PDF\n",
    "\n",
    "    Source\n",
    "    ------\n",
    "    Adapted from https://medium.com/swlh/an-empirical-view-of-marchenko-pastur-theorem-1f564af5603d\n",
    "\n",
    "    Ref\n",
    "    ---\n",
    "    Marchenko and Pastur 1967 - https://www.researchgate.net/publication/303008084_Distribution_of_eigenvalues_for_some_sets_of_random_matrices\n",
    "    \"\"\"\n",
    "\n",
    "    if len(obs.shape) == 1:\n",
    "        obs = obs.reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)\n",
    "    if x is None:\n",
    "        x = np.unique(obs).reshape(-1, 1)\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape(-1, 1)\n",
    "    logProb = kde.score_samples(x)  # log(density)\n",
    "    pdf = pd.Series(np.exp(logProb), index=x.flatten())\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def errPDFs(var, eVal, q, bWidth, pts=1000):\n",
    "    \"\"\"\n",
    "    Fit error of Empirical PDF (uses Marchenko-Pastur PDF)\n",
    "\n",
    "    Input\n",
    "    ----------\n",
    "    var: Variance (float)\n",
    "    eVal: array of eigenvalues\n",
    "    q: T/N where T is the number of rows and N the number of columns (n_neurons/n_samples)\n",
    "    bWidth: The bandwidth of the kernel\n",
    "    pts: int for the number of points used to build PDF\n",
    "    Returns\n",
    "    -------\n",
    "    A float that is the sum squared error\n",
    "\n",
    "    Source\n",
    "    ------\n",
    "    Adapted from https://medium.com/swlh/an-empirical-view-of-marchenko-pastur-theorem-1f564af5603d\n",
    "\n",
    "    Ref\n",
    "    ---\n",
    "    Marchenko and Pastur 1967 - https://www.researchgate.net/publication/303008084_Distribution_of_eigenvalues_for_some_sets_of_random_matrices\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit error\n",
    "    pdf0 = mpPDF(var, q, pts)  # theoretical pdf\n",
    "    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)  # empirical pdf\n",
    "    sse = np.sum((pdf1 - pdf0) ** 2)\n",
    "    return sse\n",
    "\n",
    "\n",
    "def findMaxEval(eVal, q, bWidth):\n",
    "    \"\"\"\n",
    "    Finds max random eigenvalue by fitting Marchenko’s dist (i.e) everything else larger than\n",
    "        this, is a signal eigenvalue\n",
    "\n",
    "    Input\n",
    "    ----------\n",
    "    eVal: array of eigenvalues to fit\n",
    "    q: T/N where T is the number of rows and N the number of columns (n_neurons/n_samples)\n",
    "    bWidth: The bandwidth of the kernel\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple containing eMax - (max eigenvalue for random eigenvalues) and var - variance attributed to noise\n",
    "\n",
    "    Source\n",
    "    ------\n",
    "    Adapted from https://medium.com/swlh/an-empirical-view-of-marchenko-pastur-theorem-1f564af5603d\n",
    "\n",
    "    Ref\n",
    "    ---\n",
    "    Marchenko and Pastur 1967 - https://www.researchgate.net/publication/303008084_Distribution_of_eigenvalues_for_some_sets_of_random_matrices\n",
    "    \"\"\"\n",
    "\n",
    "    out = minimize(lambda *x: errPDFs(*x), .5, args=(eVal, q, bWidth),\n",
    "                   bounds=((1E-5, 1 - 1E-5),))\n",
    "    if out['success']:\n",
    "        var = out['x'][0]\n",
    "    else:\n",
    "        var = 1\n",
    "    eMax = var * (1 + (1. / q) ** .5) ** 2\n",
    "    return eMax, var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906e8c5",
   "metadata": {},
   "source": [
    "### ancillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11245b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    '''\n",
    "    normalize a vector x\n",
    "    \n",
    "    Input:\n",
    "    x: numpy.ndarry shape: (n, )\n",
    "    '''\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "def first_index(x, y, condition='>', order='forward'): \n",
    "    '''\n",
    "    get the first index that meets the condition\n",
    "    \n",
    "    Inputs:\n",
    "    x: numpy.ndarry shape: (n, )\n",
    "    y: numpy.ndarry shape: (n, )\n",
    "    condition: should be one of '>','>=','<','<=','=='\n",
    "    order: choose from 'forward' and 'backward', the order we look through the array\n",
    "    '''\n",
    "    \n",
    "    if order == 'forward':\n",
    "        for i in range(len(x)):\n",
    "            if condition == '>':\n",
    "                if x[i] > y[i]:\n",
    "                    return i\n",
    "            elif condition == '>=':\n",
    "                if x[i] >= y[i]:\n",
    "                    return i\n",
    "            elif condition == '<':\n",
    "                if x[i] < y[i]:\n",
    "                    return i\n",
    "            elif condition == '<=':\n",
    "                if x[i] <= y[i]:\n",
    "                    return i\n",
    "            else:\n",
    "                if x[i] == y[i]:\n",
    "                    return i\n",
    "        return len(x)-1\n",
    "    else:\n",
    "        for i in range(len(x)-1, -1, -1):\n",
    "            if condition == '>':\n",
    "                if x[i] > y[i]:\n",
    "                    return i\n",
    "            elif condition == '>=':\n",
    "                if x[i] >= y[i]:\n",
    "                    return i\n",
    "            elif condition == '<':\n",
    "                if x[i] < y[i]:\n",
    "                    return i\n",
    "            elif condition == '<=':\n",
    "                if x[i] <= y[i]:\n",
    "                    return i\n",
    "            else:\n",
    "                if x[i] == y[i]:\n",
    "                    return i\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9457e7c",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b805fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_spike(dffs, nbins=15, std=1):\n",
    "    '''\n",
    "    get spike train from dffs.\n",
    "    detect one spike as long as its dff value exceed 2 std above mean\n",
    "    \n",
    "    Inputs:\n",
    "    dffs: numpy.ndarry cell_num * frames\n",
    "    nbins: gaussian smooth bins\n",
    "    std: gaussian smooth std\n",
    "    \n",
    "    Return:\n",
    "    spike: spike train\n",
    "    '''# dffs: [cell_id, frames]\n",
    "    dffs = copy.deepcopy(dffs)\n",
    "#     dffs = gaussian_filter1d(dffs, sigma=std)\n",
    "    spike = np.where(dffs>(np.mean(dffs, axis=1)+2*np.std(dffs, axis=1)).reshape(dffs.shape[0], 1), 1, 0)\n",
    "    return spike\n",
    "\n",
    "def get_rest(velocity, activity, rest_speed=2):  \n",
    "    '''\n",
    "    get rest periods v<2cm/s\n",
    "    \n",
    "    Input:\n",
    "    velocity: speed over [frames] \n",
    "    activity: dF/F over [cell_id, frames]\n",
    "    '''\n",
    "    return activity[:, np.where(velocity < rest_speed)[0]]\n",
    "\n",
    "def gaussian_smooth(dffs, nbins=15, std=1): \n",
    "    '''\n",
    "    smooth inputs with a gaussian kernel\n",
    "    \n",
    "    Inputs:\n",
    "    dffs: to be smooted, dF/F over [cell_id, frame]\n",
    "    nbins: length of gaussian kernel\n",
    "    std: std of gaussian kernel\n",
    "    \n",
    "    Return:\n",
    "    smooth_dffs: dffs after smoothing\n",
    "    '''\n",
    "    cell_num = dffs.shape[0]\n",
    "    bins = int((nbins-1)/2)\n",
    "    gaussian_win = gaussian(nbins, std=std)\n",
    "    gaussian_win /= np.sum(gaussian_win)\n",
    "    smooth_dffs = np.zeros(dffs.shape)\n",
    "    dffs_com = np.concatenate([np.zeros((cell_num, bins)), dffs, np.zeros((cell_num, bins+1))], axis=1)\n",
    "    for t in range(dffs.shape[1]):\n",
    "        smooth_dffs[:, t] = (dffs_com[:, t:t+2*bins+1] @ gaussian_win.reshape(nbins, 1)).reshape(cell_num)\n",
    "    return smooth_dffs\n",
    "\n",
    "def pos_dff(dffs, locations, speed, pos_bins=2):\n",
    "    '''\n",
    "    get average activity at each position bin (only consider running periods)\n",
    "    \n",
    "    Inputs:\n",
    "    dffs: [cell_id, frame]\n",
    "    locations: location over [frame]\n",
    "    speed: speed over [frame]\n",
    "    pos_bins: bin size of position\n",
    "    \n",
    "    '''\n",
    "    cell_num = dffs.shape[0]\n",
    "    loc_max = np.max(locations)\n",
    "    loc_min = np.min(locations)\n",
    "    bins_n = int((loc_max - loc_min)/pos_bins)+1\n",
    "    dff_pos = np.zeros((cell_num, bins_n))\n",
    "\n",
    "    bins_num = np.zeros(bins_n)\n",
    "    for k in range(dffs.shape[1]):\n",
    "        if speed[k] >= 2:\n",
    "            dff_pos[:, int((locations[k]-loc_min)/pos_bins)] += dffs[:, k]\n",
    "            bins_num[int((locations[k]-loc_min)/pos_bins)] += 1\n",
    "            bins_num[bins_num==0] = 1\n",
    "    for cell in range(cell_num):\n",
    "        dff_pos[cell, :] = dff_pos[cell, :] / bins_num\n",
    "    return dff_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b34056",
   "metadata": {},
   "source": [
    "### HSE detection and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbf8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_HSE(activity, spike, pcs, least_pc_num=5, min_win=150, max_win=1000, \n",
    "               bin_size=1000/30, upper_bound=3, lower_bound=1, min_peak_dis=20, mov_win=1000):  \n",
    "    '''\n",
    "    detect high synchrony events\n",
    "    \n",
    "    Input:\n",
    "    activity: dF/F traces [cell, frame]\n",
    "    spike: spike train [cell, spike]\n",
    "    pcs: id of all place cells\n",
    "    least_pc_num: the least activated pc numbers if detected as a HSE\n",
    "    min_win: minimum window size of a HSE (ms)\n",
    "    max_win: maximum window size of a HSE (ms)\n",
    "    bin_size: time length of a time bin in activity and spike (ms)\n",
    "    upper_bound, lower_bound: detect HSE when population activity exceed mean+upper_bound*std, boundary is mean+lower_bound*std\n",
    "    min_peak_dis: detect two peaks as distinct when their peak distance exceeds min_peak_dis\n",
    "    mov_win: the length of moving window (bins)\n",
    "    '''\n",
    "    population = zscore(np.mean(activity, axis=0))\n",
    "    mean = np.zeros(population.shape)\n",
    "    std = np.zeros(population.shape)\n",
    "    for i in range(population.shape[0]):\n",
    "        mean[i] = np.mean(population[i-mov_win//2:i+mov_win//2])\n",
    "        std[i] = np.std(population[i-mov_win//2:i+mov_win//2])\n",
    "\n",
    "#     plt.plot(population)\n",
    "#     plt.plot(mean+upper_bound*std)\n",
    "#     plt.plot(mean+lower_bound*std)\n",
    "#     plt.plot(mean)\n",
    "#     plt.show()\n",
    "\n",
    "    start = None\n",
    "    peak_all = []\n",
    "    t = 0\n",
    "    while t < activity.shape[1]:\n",
    "        start = first_index(population[t:], mean[t:]+lower_bound*std[t:], '>=') + t\n",
    "        exceed1 = first_index(population[start:], mean[start:]+upper_bound*std[start:], '>=') + start\n",
    "        exceed2 = first_index(population[exceed1+1:], mean[exceed1+1:]+upper_bound*std[exceed1+1:], '<=') + exceed1 + 1\n",
    "        end = first_index(population[exceed2:], mean[exceed2:]+lower_bound*std[exceed2:], '<=') + exceed2\n",
    "        if exceed1 == exceed2:\n",
    "            if population[exceed1] >= mean[exceed1]+upper_bound*std[exceed1]:\n",
    "                peak_all.append(int(exceed1))\n",
    "        else:\n",
    "            sign = np.sign(population[exceed1+1: exceed2] - population[exceed1: exceed2-1])\n",
    "            if len(sign) == 1:\n",
    "                if population[exceed1+1]>=mean[exceed1+1]+upper_bound*std[exceed1+1]:\n",
    "                    peak_all.append(int(exceed1)+1)\n",
    "            else:\n",
    "                peaks = np.where((sign[1:]-sign[:len(sign)-1])<0)[0] + 1 + exceed1 + 1\n",
    "                for peak in peaks:\n",
    "                    if population[peak]>=mean[peak]+upper_bound*std[peak]:\n",
    "                        peak_all.append(peak)\n",
    "        t = end + 1\n",
    "    \n",
    "    peak_all = np.sort(np.array(list(peak_all), dtype=np.int64))\n",
    "    peak_all = peak_all[np.where((peak_all[1:]-peak_all[:len(peak_all)-1])>min_peak_dis)[0]+1]\n",
    "    \n",
    "    HSE_events = []\n",
    "    HSE_peaks = []\n",
    "    max_win_bins = int(max_win/bin_size)+1\n",
    "    for peak in peak_all:\n",
    "        assert population[peak] >=  mean[peak]+upper_bound*std[peak]\n",
    "        start = first_index(population[: peak], mean[:peak]+lower_bound*std[:peak], '<=', order='backward')\n",
    "        end = first_index(population[peak: ], mean[peak:]+lower_bound*std[peak:], '<=') + peak\n",
    "        if end - start >= int(min_win/bin_size):\n",
    "            if np.where(np.mean(spike[pcs[:], start:end], axis=1)>0)[0].shape[0] >= least_pc_num:\n",
    "                if end - start > max_win_bins:\n",
    "                    HSE_events.append([peak-max_win_bins//2, peak+max_win_bins//2])\n",
    "                    HSE_peaks.append(peak)\n",
    "                else:\n",
    "                    HSE_events.append([start, end])\n",
    "                    HSE_peaks.append(peak)\n",
    "\n",
    "    return HSE_events, HSE_peaks, mean+lower_bound*std, mean+upper_bound*std, max_win_bins  # HSE of place cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf130e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "def plot_heatmap(aver_all, normalized, aver_center=None, lower=None, upper=None,\n",
    "                 xlabel='Frames', ylabel1='Mean dF/F (z-score)', ylabel2='Cell ID', title='Map 1'):\n",
    "    win_bins = aver_all.shape[0]\n",
    "    if aver_center is not None:\n",
    "        win = len(aver_center)\n",
    "        left = win_bins // 2 - int(win / 2)\n",
    "        right = win_bins // 2 - (win - int(win / 2))\n",
    "    fig = plt.figure(figsize=(4, 8))\n",
    "    gs = fig.add_gridspec(3, 1)\n",
    "    ax1 = fig.add_subplot(gs[0, 0], )\n",
    "    ax2 = fig.add_subplot(gs[1:, 0], sharex=ax1)\n",
    "    ax1.plot(np.arange(win_bins), aver_all)\n",
    "    if aver_center is not None:\n",
    "        ax1.plot(np.arange(left, left + win), aver_center)\n",
    "    if lower is not None:\n",
    "        ax1.plot(lower)\n",
    "    if upper is not None:\n",
    "        ax1.plot(upper)\n",
    "    im = ax2.imshow(normalized, aspect='auto', interpolation='None', cmap='cividis', )\n",
    "    ax1.set_title(title, fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    ax1.set_ylabel(ylabel1, fontsize=15)\n",
    "    ax2.set_ylabel(ylabel2, fontsize=15)\n",
    "    ax2.set_xlabel(xlabel, fontsize=15)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "    fig.align_ylabels()\n",
    "    cbar = plt.colorbar(im, ax=[ax1, ax2], )\n",
    "    cbar.ax.set_ylabel('Normalized dF/F', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def HSE_heatmap(HSE_events, HSE_peaks, activity, lower, upper, gaussian_bins=7, gaussian_std=10,\n",
    "                win_bins=100, win=30, sort='value', plot_all=False, peaks=None, pcs=None, if_rank=False,\n",
    "                plot_aver=False,\n",
    "                if_zscore=False, plot_cells=None, cell_order=None):  # activity: after smooth and zscore\n",
    "    #     activity = zscore(gaussian_filter(activity, sigma=gaussian_std))\n",
    "    mean_hse_mean = np.zeros(win_bins)\n",
    "    #     population = zscore(np.mean(activity, axis=0))\n",
    "    if not if_zscore:\n",
    "        population = zscore(np.mean(zscore(activity, axis=1), axis=0))\n",
    "    else:\n",
    "        population = zscore(np.mean(activity, axis=0))\n",
    "\n",
    "    if plot_cells is not None:\n",
    "        activity = activity[plot_cells[:], :]\n",
    "    hse_mean = np.zeros((activity.shape[0], win_bins))\n",
    "    cell_num = activity.shape[0]\n",
    "\n",
    "    ## HSE_peaks\n",
    "    num = 0\n",
    "    good_hse = 0\n",
    "    for peak in HSE_peaks:\n",
    "        if activity[:, peak - win_bins // 2:peak + win_bins // 2].shape[1] == win_bins:\n",
    "            left = peak - win_bins // 2\n",
    "            right = peak + win_bins // 2\n",
    "            hse_mean += activity[:, left:right]\n",
    "            mean_hse_mean += population[left:right]\n",
    "            num += 1\n",
    "            normalized = np.zeros(activity[:, left:right].shape)\n",
    "            for cell in range(cell_num):\n",
    "                normalized[cell, :] = normalize(activity[cell, left:right])\n",
    "            if cell_order is None:\n",
    "                if sort == 'value':\n",
    "                    # sort by peak value\n",
    "                    cell_rank = np.argsort(\n",
    "                        np.max(normalized[:, win_bins // 2 - win // 2:win_bins // 2 + win // 2], axis=1))\n",
    "                elif sort == 'peak':\n",
    "                    if peaks is None:\n",
    "                        peaks = np.argsort(\n",
    "                            np.max(normalized[:, win_bins // 2 - win // 2:win_bins // 2 + win // 2], axis=1))\n",
    "                    cell_rank = np.argsort(peaks)\n",
    "                else:  # sort == 'pc_peak'\n",
    "                    # pc sort by field location\n",
    "                    non_pl = np.setdiff1d(np.arange(cell_num), pcs)\n",
    "                    cell_rank = np.concatenate(\n",
    "                        [non_pl[np.argsort(peaks[non_pl[:]])[:]], pcs[np.argsort(peaks[pcs[:]])]])\n",
    "                if if_rank:\n",
    "                    normalized = normalized[cell_rank[::-1], :]\n",
    "\n",
    "            else:\n",
    "                if if_rank:\n",
    "                    normalized = normalized[cell_order, :]\n",
    "\n",
    "            if plot_all:\n",
    "                plot_heatmap(population[left:right], normalized, lower=lower[left:right], upper=upper[left:right],\n",
    "                             aver_center=population[peak - win // 2:peak + win // 2])\n",
    "                if good_HSE(population, peak, lower, upper):\n",
    "                    print('This is a good HSE')\n",
    "                    good_hse += 1\n",
    "                else:\n",
    "                    print('This is not a good HSE')\n",
    "\n",
    "    hse_mean /= num\n",
    "    mean_hse_mean /= num\n",
    "    normalized = np.zeros(hse_mean.shape)\n",
    "    for cell in range(cell_num):\n",
    "        normalized[cell, :] = normalize(hse_mean[cell, :])\n",
    "    if cell_order is None:\n",
    "        if sort == 'value':\n",
    "            # sort by peak value\n",
    "            cell_rank = np.argsort(np.max(normalized[:, win_bins // 2 - win // 2:win_bins // 2 + win // 2], axis=1))\n",
    "        elif sort == 'peak':\n",
    "            if peaks is None:\n",
    "                peaks = np.argmax(normalized[:, win_bins // 2 - win // 2:win_bins // 2 + win // 2], axis=1)\n",
    "            cell_rank = np.argsort(peaks)\n",
    "        else:  # sort == 'pc_peak'\n",
    "            # pc sort by field location\n",
    "            non_pl = np.setdiff1d(np.arange(cell_num), pcs)\n",
    "            cell_rank = np.concatenate([non_pl[np.argsort(peaks[non_pl[:]])[:]], pcs[np.argsort(peaks[pcs[:]])]])\n",
    "    else:\n",
    "        cell_rank = cell_order\n",
    "    if if_rank:\n",
    "        normalized = normalized[cell_rank[::-1], :]\n",
    "\n",
    "    if plot_aver:\n",
    "        plot_heatmap(mean_hse_mean, normalized, aver_center=None)\n",
    "\n",
    "    return [mean_hse_mean, lower, upper, normalized], cell_rank[::-1], good_hse\n",
    "\n",
    "\n",
    "def plot_pre_behav_post(list1, list2, list3, rank):\n",
    "    '''\n",
    "    Plot three plots on pre, hebav and post periods, each consists HSE mean of means, heatmap of HSE mean over [cell, frames]\n",
    "    \n",
    "    Input:\n",
    "    list1, list2, list3: [mean of means, lower_bound, upper_bound, heatmap] of pre, behav, post\n",
    "    rank: the order of cells\n",
    "    \n",
    "    '''\n",
    "    win_bins = list1[0].shape[0]\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = fig.add_gridspec(3, 3)\n",
    "    ax1 = fig.add_subplot(gs[0, 0], )\n",
    "    ax2 = fig.add_subplot(gs[1:, 0], sharex=ax1)\n",
    "    ax3 = fig.add_subplot(gs[0, 1], sharey=ax1)\n",
    "    ax4 = fig.add_subplot(gs[1:, 1], sharex=ax3)\n",
    "    ax5 = fig.add_subplot(gs[0, 2], sharey=ax1)\n",
    "    ax6 = fig.add_subplot(gs[1:, 2], sharex=ax5)\n",
    "\n",
    "    ax1.plot(np.arange(win_bins), list1[0])\n",
    "    ax1.plot(np.ones(win_bins) * (list1[1]))\n",
    "    ax1.plot(np.ones(win_bins) * (list1[2]))\n",
    "    im = ax2.imshow(list1[3][rank[:], :], aspect='auto', interpolation='None', cmap='cividis', )\n",
    "    ax1.set_title('Pre', fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    ax1.set_ylabel('Mean dF/F (z-score)', fontsize=15)\n",
    "    ax2.set_ylabel('Cell ID', fontsize=15)\n",
    "    ax2.set_xlabel('Frames', fontsize=15)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "    fig.align_ylabels()\n",
    "\n",
    "    #     cbar = plt.colorbar(im, ax=[ax1,ax2], )\n",
    "    #     cbar.ax.set_ylabel('Normalized dF/F',fontsize = 15)\n",
    "\n",
    "    ax3.plot(np.arange(win_bins), list2[0])\n",
    "    ax3.plot(np.ones(win_bins) * (list2[1]))\n",
    "    ax3.plot(np.ones(win_bins) * (list2[2]))\n",
    "    im = ax4.imshow(list2[3][rank[:], :], aspect='auto', interpolation='None', cmap='cividis', )\n",
    "    ax3.set_title('Behav', fontsize=20)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    ax4.set_xlabel('Frames', fontsize=15)\n",
    "    ax3.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax4.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.setp(ax4.get_yticklabels(), visible=False)\n",
    "\n",
    "    #     fig.align_ylabels()\n",
    "    #     cbar = plt.colorbar(im, ax=[ax3,ax4], )\n",
    "    #     cbar.ax.set_ylabel('Normalized dF/F',fontsize = 15)\n",
    "\n",
    "    ax5.plot(np.arange(win_bins), list3[0])\n",
    "    ax5.plot(np.ones(win_bins) * (list3[1]))\n",
    "    ax5.plot(np.ones(win_bins) * (list3[2]))\n",
    "    im = ax6.imshow(list3[3][rank[:], :], aspect='auto', interpolation='None', cmap='cividis', )\n",
    "    ax5.set_title('Post', fontsize=20)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    ax6.set_xlabel('Frames', fontsize=15)\n",
    "    ax5.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax6.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.setp(ax6.get_yticklabels(), visible=False)\n",
    "\n",
    "    #     fig.align_ylabels()\n",
    "    cbar = plt.colorbar(im, ax=[ax1, ax2, ax3, ax4, ax5, ax6], )\n",
    "    cbar.ax.set_ylabel('Normalized dF/F', fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def HSE_counts_overtime(pre_HSE_peaks, post_HSE_peaks, total_time, bin_size=1800):  \n",
    "    '''\n",
    "    Plots HSE number as a function of time window\n",
    "    \n",
    "    Inputs:\n",
    "    pre_HSE_peaks: HSE peaks in pre\n",
    "    post_HSE_peaks: HSE peaks in post\n",
    "    total_time: in frames\n",
    "    bin_size: frame size of one time window\n",
    "    '''\n",
    "    # the center/peak of each HSE events\n",
    "    nbins = total_time // bin_size + 1\n",
    "    pre = np.zeros(nbins[0])\n",
    "    post = np.zeros(nbins[1])\n",
    "    for peak in pre_HSE_peaks:\n",
    "        pre[peak // bin_size] += 1\n",
    "    for peak in post_HSE_peaks:\n",
    "        post[peak // bin_size] += 1\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    gs = fig.add_gridspec(1, 1)\n",
    "    ax1 = fig.add_subplot(gs[0, 0], )\n",
    "    ax1.plot(np.arange(nbins[0]), pre)\n",
    "    ax1.plot(np.arange(nbins[0]), np.ones(nbins[0])*np.mean(pre))\n",
    "    ax1.plot(np.arange(nbins[1]) + nbins[0], post)\n",
    "    ax1.plot(np.arange(nbins[1]) + nbins[0], np.ones(nbins[1])*np.mean(post))\n",
    "    #     ax1.set_title('Map 1', fontsize = 20)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    ax1.set_ylabel('HSE counts', fontsize=15)\n",
    "    ax1.xaxis.set_minor_locator(mticker.FixedLocator((nbins[0] // 2, nbins[1] // 2 + nbins[0])))\n",
    "    ax1.xaxis.set_minor_formatter(mticker.FixedFormatter((\"Pre\", \"Post\")))\n",
    "    plt.setp(ax1.xaxis.get_minorticklabels(), size=20, va=\"center\")\n",
    "    ax1.tick_params(\"x\", which=\"minor\", pad=25, left=False)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def plot_PC_corr(pre_dffs, post_dffs, pc_peak, slid_bins=4, dis_bin=2, order='circulate'):\n",
    "    '''\n",
    "    plot correlation between place cells' dF/F as a function of peak distance, time periods could be pre/post or HSE\n",
    "    \n",
    "    Input: \n",
    "    pre_dffs: [cell_id, frame] dF/F of all place cells in pre\n",
    "    post_dffs: [cell_id, frame] dF/F of all place cells in post\n",
    "    pc_peak: the peaks of all place cells\n",
    "    dis_bin: bin size of distance between cell peaks\n",
    "    slid_bins: bins number of a sliding window\n",
    "    '''\n",
    "    # dffs: [cell_id, activity]  pl_peak:[place field center(cm)]  one-to-one\n",
    "    pc_num = len(pc_peak)\n",
    "    pre_corr = np.corrcoef(pre_dffs)\n",
    "    post_corr = np.corrcoef(post_dffs)\n",
    "    \n",
    "    if order == 'circulate':\n",
    "        nbins = int(((np.max(pc_peak)-np.min(pc_peak))//dis_bin+1)//2+1)\n",
    "    elif order == 'sequence':\n",
    "        nbins = int(((np.max(pc_peak)-np.min(pc_peak))//dis_bin+1))\n",
    "    pre_all = [[] for i in range(nbins)]\n",
    "    post_all = [[] for i in range(nbins)]\n",
    "    for i in range(len(pc_peak)):\n",
    "        for j in range(i):\n",
    "            idx = int(abs(pc_peak[i]-pc_peak[j])//dis_bin)\n",
    "            if order == 'circulate':\n",
    "                idx = idx if idx <= nbins-idx else nbins-idx\n",
    "            pre_all[idx].append(pre_corr[i, j])\n",
    "            post_all[idx].append(post_corr[i, j])\n",
    "    \n",
    "    pre_all = np.array(pre_all, dtype=np.ndarray)\n",
    "    post_all = np.array(post_all, dtype=np.ndarray)\n",
    "    for k in range(nbins):\n",
    "        pre_all[k] = np.array(pre_all[k])\n",
    "        post_all[k] = np.array(post_all[k])\n",
    "    pre_func = np.zeros(nbins)\n",
    "    post_func = np.zeros(nbins)\n",
    "    pre_bootstrap_l = np.zeros(nbins)\n",
    "    pre_bootstrap_h = np.zeros(nbins)\n",
    "    post_bootstrap_l = np.zeros(nbins)\n",
    "    post_bootstrap_h = np.zeros(nbins)\n",
    "    #### compute mean with a sliding window on each bin\n",
    "    #### compute 95% bootstrap\n",
    "    \n",
    "    for k in range(slid_bins//2):\n",
    "        data = []\n",
    "        for item in pre_all[:k+slid_bins//2]:\n",
    "            data.extend(item)\n",
    "        data = np.array(data)\n",
    "        pre_func[k] = np.mean(data)\n",
    "        bs = bootstrap((data, ), np.mean, method='percentile', vectorized=False)\n",
    "        pre_bootstrap_h[k], pre_bootstrap_l[k] = bs.confidence_interval\n",
    "        \n",
    "        data = []\n",
    "        for item in post_all[:k+slid_bins//2]:\n",
    "            data.extend(item)\n",
    "        data = np.array(data)\n",
    "        post_func[k] = np.mean(data)\n",
    "        bs = bootstrap((data, ), np.mean, method='percentile', vectorized=False)\n",
    "        post_bootstrap_h[k], post_bootstrap_l[k] = bs.confidence_interval\n",
    "    \n",
    "    for k in range(nbins-slid_bins//2, nbins, 1):\n",
    "        data = []\n",
    "        for item in pre_all[k-slid_bins//2:]:\n",
    "            data.extend(item)\n",
    "        data = np.array(data)\n",
    "        pre_func[k] = np.mean(data)\n",
    "        bs = bootstrap((data, ), np.mean, method='percentile', vectorized=False)\n",
    "        pre_bootstrap_h[k], pre_bootstrap_l[k] = bs.confidence_interval\n",
    "        \n",
    "        data = []\n",
    "        for item in post_all[k-slid_bins//2:]:\n",
    "            data.extend(item)\n",
    "        data = np.array(data)\n",
    "        post_func[k] = np.mean(data)\n",
    "        bs = bootstrap((data, ), np.mean, method='percentile', vectorized=False)\n",
    "        post_bootstrap_h[k], post_bootstrap_l[k] = bs.confidence_interval\n",
    "        \n",
    "    for k in range(slid_bins//2, nbins-slid_bins//2):\n",
    "        data = []\n",
    "        for item in pre_all[k-slid_bins//2:k+slid_bins//2]:\n",
    "            data.extend(item)\n",
    "        data = np.array(data)\n",
    "        pre_func[k] = np.mean(data)\n",
    "        bs = bootstrap((data, ), np.mean, method='percentile', vectorized=False)\n",
    "        pre_bootstrap_h[k], pre_bootstrap_l[k] = bs.confidence_interval\n",
    "        \n",
    "        data = []\n",
    "        for item in post_all[k-slid_bins//2:k+slid_bins//2]:\n",
    "            data.extend(item)\n",
    "        data = np.array(data)\n",
    "        post_func[k] = np.mean(data)\n",
    "        bs = bootstrap((data, ), np.mean, method='percentile', vectorized=False)\n",
    "        post_bootstrap_h[k], post_bootstrap_l[k] = bs.confidence_interval\n",
    "        \n",
    "    pre_func = gaussian_filter1d(pre_func, sigma=1)\n",
    "    post_func = gaussian_filter1d(post_func, sigma=1)\n",
    "\n",
    "    fig = plt.figure(figsize = (12,4))\n",
    "    gs = fig.add_gridspec(1,5)\n",
    "    ax1 = fig.add_subplot(gs[0, :2],)\n",
    "    ax2 = fig.add_subplot(gs[0, 3:],)\n",
    "    ax1.plot(np.arange(0, dis_bin*(pre_func.shape[0]), dis_bin), pre_func, label='Pre')\n",
    "    ax1.fill_between(x=np.arange(0, dis_bin*pre_func.shape[0], dis_bin), y1=pre_bootstrap_l,\n",
    "                     y2=pre_bootstrap_h, alpha=0.2)\n",
    "    ax1.plot(np.arange(0, dis_bin*pre_func.shape[0], dis_bin), post_func, label='Post')\n",
    "    ax1.fill_between(x=np.arange(0, dis_bin*pre_func.shape[0], dis_bin), y1=post_bootstrap_l,\n",
    "                     y2=post_bootstrap_h, alpha=0.2)\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Run PF \\n peak distance(mm)')\n",
    "    ax1.set_ylabel('Offline Pairwise \\n correlation coefficient')\n",
    "    \n",
    "    for k in range(len(pre_func)):\n",
    "        ax2.scatter(pre_func[k], post_func[k], color='blue', s=10)\n",
    "    ax2.scatter(np.mean(pre_func), np.mean(post_func), color='orange', s=10)\n",
    "    ax2.plot(np.mean(pre_func)*np.ones(2), [np.mean(post_func)-sem(post_func), np.mean(post_func)+sem(post_func)], color='orange')\n",
    "    ax2.plot([np.mean(pre_func)-sem(pre_func), np.mean(pre_func)+sem(pre_func)], np.mean(post_func)*np.ones(2), color='orange')\n",
    "    ax2.plot(np.linspace(min(np.min(pre_func), np.min(post_func)), max(np.max(pre_func), np.max(post_func)), 100), \n",
    "             np.linspace(min(np.min(pre_func), np.min(post_func)), max(np.max(pre_func), np.max(post_func)), 100), linestyle='--')\n",
    "    ax2.set_xlabel('Pre offline synchrony \\n (mean corrected coefficient)')\n",
    "    ax2.set_ylabel('Post offline synchrony \\n (mean corrected coefficient)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bcf7a",
   "metadata": {},
   "source": [
    "### detect modulated cell in HSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mod_cell(HSE_peaks, cell_dff, hse_win=30, win_size=100, mov_win=500, exceed_percent=0.05):\n",
    "    '''\n",
    "    test if a cell is modulated by HSE or not\n",
    "    \n",
    "    Input:\n",
    "    HSE_peaks: list, peaks of HSEs\n",
    "    cell_dff: dF/F of the cell to be tested\n",
    "    hse_win: window length of HSE\n",
    "    win_size: window length of testing period\n",
    "    mov_win: compute baseline within the moving window length\n",
    "    exceed_percent: test a cell as positive/negative if exceed_percent more/less than base line\n",
    "    \n",
    "    Output:\n",
    "    label, baseline: label=1, positive modulated; -1, negative modulated; 0, not modulated\n",
    "    '''\n",
    "    hse_means = np.zeros(len(HSE_peaks))\n",
    "    com_means = np.zeros(len(HSE_peaks))\n",
    "    com_medians = np.zeros(len(HSE_peaks))\n",
    "    for t in range(len(HSE_peaks)):\n",
    "        hse_means[t] = np.mean(cell_dff[HSE_peaks[t]-hse_win//2:HSE_peaks[t]+hse_win//2])\n",
    "        com_means[t] = np.mean(cell_dff[HSE_peaks[t] - win_size // 2:HSE_peaks[t] - win_size // 2 + hse_win])\n",
    "        com_medians[t] = np.median(cell_dff[HSE_peaks[t]-hse_win//2:HSE_peaks[t]-hse_win//2])\n",
    "    \n",
    "    wil_score = wilcoxon(hse_means, com_means)\n",
    "    baseline = np.mean(com_means)\n",
    "    if wil_score[1] > 0.05:\n",
    "#         print('p value > 0.05')\n",
    "        return 0, baseline\n",
    "    elif np.mean(hse_means) > (1 + exceed_percent) * baseline:\n",
    "#         print('positive')\n",
    "        return 1, baseline\n",
    "    elif np.mean(hse_means) < (1 - exceed_percent) * baseline:\n",
    "#         print('negative')\n",
    "        return -1, baseline\n",
    "    else:\n",
    "        return 0, baseline\n",
    "\n",
    "\n",
    "def find_mod_cell(HSE_peaks, dff, hse_win=30, win_size=100, title='', plot_all=False, plot_aver=False, exceed_percent=0.05):\n",
    "    '''\n",
    "    use test_mod_cell to test all the cells and make plots\n",
    "    \n",
    "    Input:\n",
    "    HSE_peaks: peak of all HSEs\n",
    "    dff: [cell_id, frame] dF/F\n",
    "    hse_win: window length of HSEs\n",
    "    win_size: window length of peri-HSEs\n",
    "    plot_all: True if make plots of all cells, else False\n",
    "    plot_aver: True if make plots of average, else False\n",
    "    \n",
    "    Output:\n",
    "    pos_mod_cell: list, cell id of all positive modulated cell\n",
    "    neg_mod_cell: list, cell id of all negative modulated cell\n",
    "    '''\n",
    "    pos_mod_cell = []\n",
    "    neg_mod_cell = []\n",
    "    pos_hse_mean = np.zeros((len(HSE_peaks), win_size))\n",
    "    neg_hse_mean = np.zeros((len(HSE_peaks), win_size))\n",
    "    \n",
    "    for cell in range(dff.shape[0]):\n",
    "        label, baseline = test_mod_cell(HSE_peaks, dff[cell, :], hse_win, win_size, exceed_percent=exceed_percent)\n",
    "        if label==1:\n",
    "            pos_mod_cell.append(cell)\n",
    "#             print('mod cell')\n",
    "    \n",
    "        elif label==-1:\n",
    "            neg_mod_cell.append(cell)\n",
    "#             print('not mod cell')\n",
    "        \n",
    "    for cell in pos_mod_cell:\n",
    "        cell_hse = np.zeros((len(HSE_peaks), win_size))\n",
    "        for k in range(len(HSE_peaks)):\n",
    "            cell_hse[k, :] = normalize(dff[cell, HSE_peaks[k]-win_size//2:HSE_peaks[k]+win_size//2])\n",
    "        pos_hse_mean += cell_hse\n",
    "        cell_hse = cell_hse[np.argsort(np.max(cell_hse[:, win_size//2-hse_win//2:win_size//2+hse_win//2], axis=1)), :]\n",
    "        cell_mean = np.mean(cell_hse, axis=0)\n",
    "        if plot_all:\n",
    "            plot_heatmap(cell_mean, cell_hse, ylabel2='HSE number', title=title+'Positive', \n",
    "                            lower=baseline*np.ones(len(cell_mean)), upper=baseline*np.ones(len(cell_mean)), \n",
    "                            aver_center=cell_mean[win_size//2-hse_win//2:win_size//2+hse_win//2])\n",
    "        \n",
    "    for cell in neg_mod_cell:\n",
    "        cell_hse = np.zeros((len(HSE_peaks), win_size))\n",
    "        for k in range(len(HSE_peaks)):\n",
    "            cell_hse[k, :] = normalize(dff[cell, HSE_peaks[k]-win_size//2:HSE_peaks[k]+win_size//2])\n",
    "        neg_hse_mean += cell_hse\n",
    "        cell_hse = cell_hse[np.argsort(np.max(cell_hse[:, win_size//2-hse_win//2:win_size//2+hse_win//2], axis=1)), :]\n",
    "        cell_mean = np.mean(cell_hse, axis=0)\n",
    "        if plot_all:\n",
    "            plot_heatmap(cell_mean, cell_hse, ylabel2='HSE number', title=title+'Negitive', \n",
    "                            lower=baseline*np.ones(len(cell_mean)), upper=baseline*np.ones(len(cell_mean)), \n",
    "                            aver_center=cell_mean[win_size//2-hse_win//2:win_size//2+hse_win//2])\n",
    "        \n",
    "    pos_hse_mean /= len(pos_mod_cell)\n",
    "    neg_hse_mean /= len(neg_mod_cell)\n",
    "    if plot_aver:\n",
    "        plot_heatmap(np.mean(pos_hse_mean, axis=0), pos_hse_mean, ylabel2='HSE number', title='Positive')\n",
    "        plot_heatmap(np.mean(neg_hse_mean, axis=0), neg_hse_mean, ylabel2='HSE number', title='Negative')\n",
    "    \n",
    "    \n",
    "    return pos_mod_cell, neg_mod_cell\n",
    "    \n",
    "    \n",
    "def plot_mod_cell_num(pre_num, post_num):\n",
    "    '''\n",
    "    make bar plot of positive and negative modulated cells in pre and post HSEs\n",
    "    \n",
    "    Input:\n",
    "    pre_num: [pos_num, neg_num]\n",
    "    post_num: [pos_num, neg_num]\n",
    "    '''\n",
    "    \n",
    "    labels = ['Pre', 'Post']\n",
    "    x = np.arange(1, len(labels)+1)  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, [pre_num[0], post_num[0]], width, label='Positive')\n",
    "    rects2 = ax.bar(x + width/2, [pre_num[1], post_num[1]], width, label='Negative')\n",
    "\n",
    "    ax.set_ylabel('Cell counts')\n",
    "    plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    ax.xaxis.set_minor_locator(mticker.FixedLocator((1, 2)))\n",
    "    ax.xaxis.set_minor_formatter(mticker.FixedFormatter(labels))\n",
    "    plt.setp(ax.xaxis.get_minorticklabels())\n",
    "    ax.tick_params(axis=\"x\", which=\"minor\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a875c38",
   "metadata": {},
   "source": [
    "#### Tereda's way to detect modulated cell for each HSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d58f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tereda's\n",
    "def get_reactivated_cell(peak, dff, hse_win, win, cell_type=None, pass_corr=0.5): \n",
    "    '''\n",
    "    detect reactivated cell for each HSE\n",
    "    \n",
    "    Input:\n",
    "    peak: peak time of a HSE\n",
    "    dff: dF/F of one HSE [frames]\n",
    "    hse_win: window size of a HSE\n",
    "    win: peri-HSE window size\n",
    "    \n",
    "    Return:\n",
    "    1 for positive modulated cell; -1 for negative modulated cell; 0 for not modulated cell\n",
    "    '''\n",
    "    pos_react_cell = []\n",
    "    neg_react_cell = []\n",
    "    pca = PCA(n_components=2).fit(copy.deepcopy(dff[:, peak-hse_win//2:peak+hse_win//2]))  # +- 3s\n",
    "    pca1 = pca.components_[0]\n",
    "    for cell_id in range(dff.shape[0]):\n",
    "        cell_dff = copy.deepcopy(dff[cell_id, peak-hse_win//2:peak+hse_win//2])\n",
    "        if np.isnan(pearsonr(cell_dff, pca1)[0]):\n",
    "            continue\n",
    "        if pearsonr(cell_dff, pca1)[0] > pass_corr:\n",
    "            pos_react_cell.append(cell_id)\n",
    "        elif pearsonr(cell_dff, pca1)[0] < -pass_corr:\n",
    "            neg_react_cell.append(cell_id)\n",
    "    \n",
    "    pos_react_cell = np.array(pos_react_cell, dtype=np.int64)\n",
    "    neg_react_cell = np.array(neg_react_cell, dtype=np.int64)\n",
    "#     As = reactivated_cell[cell_type[reactivated_cell]==1]\n",
    "#     Xs = reactivated_cell[cell_type[reactivated_cell]==-1]\n",
    "#     plt.pie([len(As), len(Xs), len(reactivated_cell)-len(As)-len(Xs)], labels=['A', 'X', 'Other'], autopct='%1.2f%%')\n",
    "#     plt.show()\n",
    "    \n",
    "    hse_reac_pos = dff[pos_react_cell, peak-win//2:peak+win//2]\n",
    "    hse_reac_neg = dff[neg_react_cell, peak-win//2:peak+win//2]\n",
    "    for cell in range(len(pos_react_cell)):\n",
    "        hse_reac_pos[cell, :] = normalize(hse_reac_pos[cell, :])\n",
    "    for cell in range(len(neg_react_cell)):\n",
    "        hse_reac_neg[cell, :] = normalize(hse_reac_neg[cell, :])\n",
    "    hse_reac_pos = hse_reac_pos[np.argsort(np.argmax(hse_reac_pos[:, win//2-hse_win//2:win//2+hse_win//2], axis=1)), :]\n",
    "    hse_reac_neg = hse_reac_neg[np.argsort(np.argmax(hse_reac_neg[:, win//2-hse_win//2:win//2+hse_win//2], axis=1)), :]\n",
    "    hse_reac = np.concatenate((hse_reac_pos, hse_reac_neg), axis=0)\n",
    "    plot_heatmap(np.mean(hse_reac, axis=0), hse_reac)\n",
    "    plot_heatmap(np.mean(hse_reac_pos, axis=0), hse_reac_pos)\n",
    "    plot_heatmap(np.mean(hse_reac_neg, axis=0), hse_reac_neg)\n",
    "    \n",
    "#     hse_reac = dff[As, peak-win//2:peak+win//2]\n",
    "#     for cell in range(len(As)):\n",
    "#         hse_reac[cell, :] = normalize(hse_reac[cell, :])\n",
    "#     rank = np.argsort(np.argmax(hse_reac[:, win//2-hse_win//2:win//2+hse_win//2], axis=1))\n",
    "#     hse_reac = hse_reac[rank, :]\n",
    "#     plot_heatmap(np.mean(hse_reac, axis=0), hse_reac)\n",
    "    \n",
    "#     hse_reac = dff[Xs, peak-win//2:peak+win//2]\n",
    "#     for cell in range(len(Xs)):\n",
    "#         hse_reac[cell, :] = normalize(hse_reac[cell, :])\n",
    "#     rank = np.argsort(np.argmax(hse_reac[:, win//2-hse_win//2:win//2+hse_win//2], axis=1))\n",
    "#     hse_reac = hse_reac[rank, :]\n",
    "#     plot_heatmap(np.mean(hse_reac, axis=0), hse_reac)\n",
    "    \n",
    "    return hse_reac_pos, hse_reac_neg#, As, Xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafffb03",
   "metadata": {},
   "source": [
    "### reactivation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grosmark \n",
    "\n",
    "def reactivation_strength(ICA_matrix, activity_matrix):\n",
    "    '''\n",
    "    Compute reactivation strength matrix\n",
    "    \n",
    "    ICA_matrix: [cell_id, assembly_id] weights\n",
    "    activity_matrix: [cell_id, frames] dF/F\n",
    "    '''\n",
    "    cell_num, assem_num = ICA_matrix.shape\n",
    "    R = np.zeros((assem_num, activity_matrix.shape[1]))  ## reactivation strength\n",
    "    for i in range(ICA_matrix.shape[1]): #there is probably a better way to code this but fuck it \n",
    "        weighted = (activity_matrix.transpose()*ICA_matrix[:,i]).transpose()\n",
    "        R[i] = np.sum(weighted, axis=0)**2\n",
    "    return R\n",
    "\n",
    "\n",
    "def PCC_score(x, ICA_matrix, activity_matrix):  # xth cell\n",
    "    '''\n",
    "    compute PCC score for one cell\n",
    "    \n",
    "    Input:\n",
    "    x: id of cell\n",
    "    ICA_matrix: [cell_id, assembly_id] weights\n",
    "    activity_matrix: [cell_id, frames] dF/F\n",
    "    '''\n",
    "    assert type(x) == int\n",
    "    R1 = reactivation_strength(ICA_matrix, activty_matrix)\n",
    "    R2 = reactivation_strength(ICA_matrix[np.arange(ICA_matrix.shape[0]) != x, :],\n",
    "                               activity_matrix[np.arange(ICA_matrix.shape[0]) != x, :])\n",
    "    return np.mean(R1 - R2)\n",
    "\n",
    "\n",
    "def assembly_activation_strength(ICA_matrix, pre_hse_matrix, post_hse_matrix, hse_win, plot_num=None):\n",
    "    '''\n",
    "    compute activation strength for each assembly and make plots\n",
    "    \n",
    "    Inputs:\n",
    "    ICA_matrix: [cell_id, assembly_id] weights\n",
    "    pre_hse_matrix: [cell_id, frames] dF/F of all HSEs in pre\n",
    "    post_hse_matrix: [cell_id, frames] dF/F of all HSEs in post\n",
    "    hse_win: window length of HSE\n",
    "    plot_num: if int, make the plot of plot_num(th) assembly; if 'plot_average', make the average plot of all assemblies\n",
    "    '''\n",
    "    # hse_matrix: 1s length\n",
    "    pre_assembly_strength = zscore(reactivation_strength(ICA_matrix, pre_hse_matrix), axis=1)\n",
    "\n",
    "    post_assembly_strength = zscore(reactivation_strength(ICA_matrix, post_hse_matrix), axis=1)\n",
    "\n",
    "    peri_win = pre_hse_matrix.shape[1]\n",
    "\n",
    "    if type(plot_num) == int:\n",
    "        fig = plt.figure(figsize=(12, 4))\n",
    "        gs = fig.add_gridspec(1, 2)\n",
    "        ax1 = fig.add_subplot(gs[0, 0], )\n",
    "        pre_strength_smooth = gaussian_filter1d(pre_assembly_strength[plot_num, :], sigma=5)\n",
    "        post_strength_smooth = gaussian_filter1d(post_assembly_strength[plot_num, :], sigma=5)\n",
    "        ax1.plot(pre_strength_smooth, label='Pre')\n",
    "        ax1.plot(post_strength_smooth, label='Post')\n",
    "        ax1.set_title('Rum ensemble ' + str(plot_num), fontsize=20)\n",
    "        plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "        ax1.set_ylabel('Run ensemble reactivation', fontsize=15)\n",
    "        ax1.set_xlabel('Peri-HSE time(s)')\n",
    "        ax1.tick_params(axis='y', which='major', labelsize=12)\n",
    "        ax1.legend()\n",
    "        #         ax1.xaxis.set_minor_locator(mticker.FixedLocator((0, post_hse_matrix.shape[1]//2, post_hse_matrix.shape[1])))\n",
    "        #         ax1.xaxis.set_minor_formatter(mticker.FixedFormatter((-0.5, 0, 0.5)))\n",
    "        #         plt.setp(ax1.xaxis.get_minorticklabels(), size=20, va=\"center\")\n",
    "        #         ax1.tick_params(\"x\",which=\"minor\",pad=25, left=False)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 1], )\n",
    "        ax2.scatter(np.max(pre_strength_smooth[peri_win // 2 - hse_win // 2:peri_win // 2 + hse_win // 2]),\n",
    "                    np.max(post_strength_smooth[peri_win // 2 - hse_win // 2:peri_win // 2 + hse_win // 2]),\n",
    "                    marker='+')\n",
    "        ax2.plot(np.arange(0, 2, 0.1), np.arange(0, 2, 0.1), linestyle='--')\n",
    "        ax2.set_ylabel('Post within-HSE \\n run ensemble reactivation', fontsize=15)\n",
    "        ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax2.set_xlabel('Pre within-HSE \\n run ensemble reactivation', fontsize=15)\n",
    "\n",
    "        plt.show()\n",
    "    elif plot_num == 'plot average':\n",
    "        fig = plt.figure(figsize=(12, 4))\n",
    "        gs = fig.add_gridspec(1, 2)\n",
    "        ax1 = fig.add_subplot(gs[0, 0], )\n",
    "        pre_population = gaussian_filter1d(np.mean(pre_assembly_strength, axis=0), sigma=5)\n",
    "        post_population = gaussian_filter1d(np.mean(post_assembly_strength, axis=0), sigma=5)\n",
    "        ax1.plot(np.arange(peri_win), pre_population, label='Pre')\n",
    "        ax1.plot(np.arange(peri_win), post_population, label='Post')\n",
    "#         ax1.plot(np.arange(peri_win // 2 - hse_win // 2, peri_win // 2 + hse_win // 2),\n",
    "#                  pre_population[peri_win // 2 - hse_win // 2:peri_win // 2 + hse_win // 2])\n",
    "#         ax1.plot(np.arange(peri_win // 2 - hse_win // 2, peri_win // 2 + hse_win // 2),\n",
    "#                  post_population[peri_win // 2 - hse_win // 2:peri_win // 2 + hse_win // 2])\n",
    "        ax1.set_title('Rum ensemble ' + str(plot_num), fontsize=20)\n",
    "        plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "        ax1.legend()\n",
    "        ax1.set_ylabel('Run ensemble reactivation', fontsize=15)\n",
    "        ax1.tick_params(axis='y', which='major', labelsize=12)\n",
    "        ax1.legend()\n",
    "        #         ax1.xaxis.set_minor_locator(mticker.FixedLocator((0, post_hse_matrix.shape[1]//2, post_hse_matrix.shape[1])))\n",
    "        #         ax1.xaxis.set_minor_formatter(mticker.FixedFormatter((-0.5, 0, 0.5)))\n",
    "        #         plt.setp(ax1.xaxis.get_minorticklabels(), size=20, va=\"center\")\n",
    "        #         ax1.tick_params(\"x\",which=\"minor\",pad=50, left=False)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 1], )\n",
    "        ax2.scatter(np.max(pre_population[peri_win // 2 - hse_win // 2:peri_win // 2 + hse_win // 2]),\n",
    "                    np.max(post_population[peri_win // 2 - hse_win // 2:peri_win // 2 + hse_win // 2]), marker='+')\n",
    "        ax2.plot(np.arange(0, 2, 0.1), np.arange(0, 2, 0.1), linestyle='--')\n",
    "        ax2.set_ylabel('Post within-HSE \\n run ensemble reactivation', fontsize=15)\n",
    "        ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax2.set_xlabel('Pre within-HSE \\n run ensemble reactivation', fontsize=15)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5b12a",
   "metadata": {},
   "source": [
    "### Assembly analysis (Edgar's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87761b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EB_assembly_analysis(data):\n",
    "    # How to use\n",
    "\n",
    "    #1) Create spike matrix \n",
    "\n",
    "    bin_size = 0.025 #in seconds \n",
    "\n",
    "    # bin_size=0.025\n",
    "    matrix = binning(data, 25) #deconvolved data #this is the normal spike matrix (np.array with shape(neurons,time bins))\n",
    "    print('binning finished')\n",
    "    matrix = gaussian_filter1d(matrix, sigma=1)\n",
    "    print('Smoothing finished')\n",
    "    #we can consider smoothing it \n",
    "#     s_matrix = np.zeros_like(matrix)\n",
    "#     for i in range(matrix.shape[0]): \n",
    "#         s_matrix[i] = gaussian_filter1d(matrix[i], sigma = 1)\n",
    "    #if you'd like to use the smoothed one just change matrix to s_matrix (or assign matrix as s_matrix)\n",
    "\n",
    "    #Next z-score such that we have null-mean and unity-variance \n",
    "    z_matrix = stats.zscore(matrix, axis=1) #this is the z-scored spike matrix (z-scoring is on full matrix on purpose! - relative to baseline would require different math)\n",
    "    print('zscore finished')\n",
    "    covariance_matrix = np.cov(z_matrix) #this is the covariance matrix\n",
    "\n",
    "\n",
    "    #2) Find number of cell assemblies using Marchenko Pastur Theorem\n",
    "\n",
    "    # Getting Eigenvalues and Eigenvectors\n",
    "    eig_vals, eig_vecs = np.linalg.eig(covariance_matrix) ####\n",
    "    print('Got eigenvalues and eigenvectors')\n",
    "    #determining q -> Ncol/NRow of spike matrix for estimating Randomness using Marchenko-Pastur\n",
    "    q = z_matrix.shape[1]/z_matrix.shape[0]\n",
    "\n",
    "    # Getting Max Eigenvalues and calculating variance attributed to noise\n",
    "    eMax0, var0 = findMaxEval(eig_vals, q, bWidth=0.01)\n",
    "\n",
    "    #determining number of significant assemblies \n",
    "    sig_eigs = eig_vals[np.where(eig_vals > eMax0)]\n",
    "    print('Got number of significant assemblies')\n",
    "    \n",
    "    #3) Extraction of cell assembly patterns and estimation of cell assembly activity\n",
    "\n",
    "    X = z_matrix #just assigning matrix to new variable in case ICA transforms the array \n",
    "\n",
    "    #Lopes dos Santos et al. 2013 says to first reduce PCA and run ICA in that reduced space.\n",
    "    #I tried that and it gives essentially the same result but sometimes inversts the sign of the weight. \n",
    "    #I suspect that the sklearn ICA function already factors this in when you give it the number of components to pull out. \n",
    "    #So I just run ICA straight as it seems to give me reasonable results\n",
    "\n",
    "    # #uncomment if like to run PCA first \n",
    "    # pca = PCA(n_components=len(sig_eigs), svd_solver='arpack', random_state=101)\n",
    "    # X = pca.fit_transform(X)\n",
    "\n",
    "    #we run fastICA (Hyvarinen and Oja 2000 - implemented by sklearn A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430.) \n",
    "    #this attributes wieghts to each neuron for the number of determined cell assemblies \n",
    "    transformer = FastICA(n_components=len(sig_eigs), whiten=True, random_state=101) \n",
    "    X_transformed = transformer.fit_transform(X) \n",
    "    print('fastICA finished')\n",
    "\n",
    "#     #Next we calculate the assembly strength by multiplying the spike matrix by each neuron's ICA weight for each assembly pattern\n",
    "#     #We then sum these products to obtain the time resolved activation strength for each assembly\n",
    "#     assembly_strength = np.zeros((X_transformed.shape[1], z_matrix.shape[1])) #this will be time resolved assembly strength (np.array with shape(assemblies, time_bins))\n",
    "#     for i in range(X_transformed.shape[1]): #there is probably a better way to code this but fuck it \n",
    "#         weighted = (z_matrix.transpose()*X_transformed[:,i]).transpose()\n",
    "#         assembly_strength[i] = np.sum(weighted, axis=0)\n",
    "    \n",
    "    assembly_strength = reactivation_strength(X_transformed, z_matrix)\n",
    "    print('Computed assembly strength')\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "    ax1 = fig.add_subplot(gs[0, 0], )\n",
    "    ax2 = fig.add_subplot(gs[0, 1], )\n",
    "    ax3 = fig.add_subplot(gs[1, 0], )\n",
    "    ax4 = fig.add_subplot(gs[1, 1], )\n",
    "\n",
    "    show_z = np.zeros(z_matrix.shape)\n",
    "    for i in range(show_z.shape[0]):\n",
    "        show_z[i, :] = normalize(z_matrix[i, :])\n",
    "    im = ax1.imshow(show_z, aspect='auto', interpolation='None', cmap='cividis')\n",
    "    ax1.set_title('Z-scored Spike Matrix' , fontsize=20)\n",
    "    ax1.set_ylabel('Neuron', fontsize=15)\n",
    "    ax1.set_xlabel('Time(0.025s bins)', fontsize=15)\n",
    "    cbar = plt.colorbar(im, ax=ax1, )\n",
    "    cbar.ax.set_ylabel('Nomalized(Z score)', fontsize=15)\n",
    "\n",
    "    im = ax2.imshow(covariance_matrix, aspect='auto', interpolation='None', cmap='cividis')\n",
    "    ax2.set_title('Covariance Matrix' , fontsize=20)\n",
    "    ax2.set_ylabel('Neuron', fontsize=15)\n",
    "    ax2.set_xlabel('Neuron', fontsize=15)\n",
    "    cbar = plt.colorbar(im, ax=ax2, )\n",
    "    cbar.ax.set_ylabel('Covariance', fontsize=15)\n",
    "\n",
    "    x = np.arange(0, np.max(eig_vals)+1, 0.2)\n",
    "    eigs = np.zeros(x.shape)\n",
    "    for k in eig_vals:\n",
    "        eigs[int(k/0.2)] += 1\n",
    "    eigs /= np.sum(eigs)\n",
    "    ax3.bar(x, eigs, color='orange', label='empirical data')\n",
    "    ax3.plot(np.ones(100)*(1+np.sqrt(1/q))**2, np.linspace(0, 0.3, 100), linestyle='--', label='max lamda')\n",
    "    ax3.legend()\n",
    "    ax3.set_title('Marchenko-Pastur Theorem' , fontsize=20)\n",
    "    ax3.set_xlabel('\\lambda', fontsize=15)\n",
    "    ax3.set_ylabel('prob[\\lambda]', fontsize=15)\n",
    "    \n",
    "    im = ax4.imshow(X_transformed.T, aspect='auto', interpolation='None', cmap='cividis')\n",
    "    ax4.set_title('Assembly Contribution' , fontsize=20)\n",
    "    ax4.set_xlabel('Neuron', fontsize=15)\n",
    "    ax4.set_ylabel('Assembly', fontsize=15)\n",
    "    cbar = plt.colorbar(im, ax=ax4, )\n",
    "    cbar.ax.set_ylabel('ICA weight', fontsize=15)\n",
    "    \n",
    "    \n",
    "    ## spike train from Spks_behav\n",
    "    ## Spks_behav\n",
    "\n",
    "    #plot assembly \n",
    "\n",
    "    one = 0\n",
    "    two = 1\n",
    "    three = 2\n",
    "\n",
    "    window = np.argmax(assembly_strength[0,:])\n",
    "    size = 100\n",
    "\n",
    "    df = pd.DataFrame(X_transformed)\n",
    "    res = pd.DataFrame(df.values.argsort(0), columns=df.columns)\\\n",
    "            .iloc[len(df.index): -4: -1]\n",
    "    color_seq = [0]*len(matrix)\n",
    "    for udx, u in enumerate(matrix): \n",
    "        if udx in np.array(res.iloc[:,one]): \n",
    "            color_seq[udx] = 'C0'\n",
    "        elif udx in np.array(res.iloc[:,two]): \n",
    "            color_seq[udx] = 'C1'\n",
    "        elif udx in np.array(res.iloc[:,three]): \n",
    "            color_seq[udx] = 'C2'\n",
    "        else:\n",
    "            color_seq[udx] = 'gray'\n",
    "        \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "\n",
    "    gs = fig.add_gridspec(3,3)\n",
    "    ax2 = fig.add_subplot(gs[2, :2],)\n",
    "    ax1 = fig.add_subplot(gs[:2, :2], sharex=ax2)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "    ax5 = fig.add_subplot(gs[2, 2],)\n",
    "    ax3 = fig.add_subplot(gs[0, 2],sharex = ax5, sharey=ax5)\n",
    "    ax4 = fig.add_subplot(gs[1, 2],sharex = ax5, sharey=ax5)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for udx, u in enumerate(matrix):\n",
    "        ax1.scatter([idx for idx, i in enumerate(matrix[udx,window-size:window+size]) if i > 0], [udx]*len([idx for idx, i in enumerate(matrix[udx,window-size:window+size]) if i > 0]), s = 3, color = color_seq[udx])\n",
    "\n",
    "    ax2.plot(assembly_strength[one,window-size:window+size])\n",
    "    ax2.plot(assembly_strength[two,window-size:window+size])\n",
    "    ax2.plot(assembly_strength[three,window-size:window+size])\n",
    "\n",
    "    ax3.hlines(y=[i+1 for i in range(len(X_transformed[:,one]))], xmin=0, xmax=[i for i in X_transformed[:,one]], color='C0', alpha = 0.7)\n",
    "    ax3.plot([i for i in X_transformed[:,one]], range(len(X_transformed)) , \"o\", color = 'C0', markersize = 3)\n",
    "\n",
    "    ax4.hlines(y=[i+1 for i in range(len(X_transformed[:,two]))], xmin=0, xmax=[i for i in X_transformed[:,two]], color='C1', alpha = 0.7)\n",
    "    ax4.plot([i for i in X_transformed[:,two]], range(len(X_transformed)), \"o\", color = 'C1', markersize = 3)\n",
    "\n",
    "    ax5.hlines(y=[i+1 for i in range(len(X_transformed[:,three]))], xmin=0, xmax=[i for i in X_transformed[:,three]], color='C2', alpha = 0.7)\n",
    "    ax5.plot([i for i in X_transformed[:,three]], range(len(X_transformed)), \"o\", color = 'C2', markersize = 3)\n",
    "\n",
    "    ax5.set_xlim(-1,1)\n",
    "\n",
    "    # ax5.set_xticklabels(np.arange(-1,1.5,0.5))\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax5.set_yticks(np.arange(0,25,5))\n",
    "    ax1.set_yticks(np.arange(0,25,5))\n",
    "\n",
    "\n",
    "    ax1.set_ylabel('Neuron', fontsize = 10)\n",
    "    ax2.set_ylabel('Assembly \\n Strength (z)', fontsize = 10)\n",
    "    ax2.set_xlabel('Time ({} s bins)'.format(bin_size), fontsize = 10)\n",
    "\n",
    "    ax3.set_ylabel('Neuron', fontsize = 10)\n",
    "    ax4.set_ylabel('Neuron', fontsize = 10)\n",
    "    ax5.set_ylabel('Neuron', fontsize = 10)\n",
    "\n",
    "\n",
    "    ax3.set_title('Assembly 1', fontsize = 10)\n",
    "    ax4.set_title('Assembly 2', fontsize = 10)\n",
    "    ax5.set_title('Assembly 3', fontsize = 10)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return X_transformed, assembly_strength\n",
    "\n",
    "def binning(data, nbins):\n",
    "    '''\n",
    "    bin data\n",
    "    data:[cells, activity]\n",
    "    '''\n",
    "    bin_num = int(data.shape[1]/nbins)\n",
    "    binned = np.zeros((data.shape[0], bin_num))\n",
    "    for k in range(bin_num):\n",
    "        binned[:, k] = np.sum(data[:, k*nbins:(k+1)*nbins], axis=1)\n",
    "    return binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spike train, smoothing and zscoring\n",
    "pre_spike_train = get_sparse_spike(Spks_pre, nbins=15, std=2)\n",
    "# pre_smooth = gaussian_smooth(FNc_pre, nbins=15, std=1)\n",
    "pre_smooth = gaussian_filter1d(Spks_pre, sigma=2)\n",
    "pre_zscore = zscore(pre_smooth, axis=1)\n",
    "\n",
    "post_spike_train = get_sparse_spike(Spks_post, nbins=15, std=2)\n",
    "# post_smooth = gaussian_smooth(FNc_post, nbins=15, std=1)\n",
    "post_smooth = gaussian_filter1d(Spks_post, sigma=2)\n",
    "post_zscore = zscore(post_smooth, axis=1)\n",
    "\n",
    "\n",
    "# rest = get_rest(, Spks_behave)\n",
    "# rest_spike_train = get_sparse_spike(rest, nbins=15, std=1)\n",
    "# rest_smooth = gaussian_smooth(rest, nbins=15, std=1)\n",
    "# rest_zscore = zscore(rest_smooth, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb29cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HSE events\n",
    "pre_HSE, pre_HSE_peaks, pre_lower, pre_upper, pre_max_bins = detect_HSE(pre_zscore, pre_spike_train, pcs, upper_bound=3)\n",
    "post_HSE, post_HSE_peaks, post_lower, post_upper, post_max_bins = detect_HSE(post_zscore, post_spike_train, pcs, upper_bound=3)\n",
    "# rest_HSE_events, rest_HSE_peaks, rest_lower, rest_upper = detect_HSE(rest_zscore, rest_spike_train, pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_hse_dff = None\n",
    "post_hse_dff = None\n",
    "\n",
    "for event in pre_HSE:\n",
    "    if pre_hse_dff is None:\n",
    "        pre_hse_dff = pre_zscore[:, event[0]:event[1]]\n",
    "    else:\n",
    "        pre_hse_dff = np.concatenate((pre_hse_dff, pre_zscore[:, event[0]:event[1]]), axis=1)\n",
    "\n",
    "for event in post_HSE:\n",
    "    if post_hse_dff is None:\n",
    "        post_hse_dff = post_zscore[:, event[0]:event[1]]\n",
    "    else:\n",
    "        post_hse_dff = np.concatenate((post_hse_dff, post_zscore[:, event[0]:event[1]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4234771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_PC_corr(pre_zscore[pcs, :], post_zscore[pcs, :], PeakMap1[pcs], dis_bin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810735ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by peak location\n",
    "list_pre, rank = HSE_heatmap(pre_HSE, pre_HSE_peaks, pre_zscore, pre_lower, pre_upper, sort='peak', peaks=PeakMap1, pcs=pcs, win_bins=500, plot_aver=False, plot_all=True, if_rank=True)\n",
    "list_post = HSE_heatmap(post_HSE, post_HSE_peaks, post_zscore, post_lower, post_upper, sort='peak', peaks=PeakMap1, win_bins=500, plot_aver=False, plot_all=True, if_rank=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b50918",
   "metadata": {},
   "outputs": [],
   "source": [
    "HSE_counts_overtime(pre_HSE_peaks, post_HSE_peaks, np.array([pre_zscore.shape[1], post_zscore.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc33865",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_mod_cell, pre_not_mod_cell = find_mod_cell(pre_HSE_peaks, pre_zscore, title='Pre ', win_size=300, \n",
    "                                               plot_aver=False, plot_all=True, exceed_percent=0.5)\n",
    "post_mod_cell, post_not_mod_cell = find_mod_cell(post_HSE_peaks, post_zscore, title='Post ', win_size=300, \n",
    "                                                 plot_aver=False, plot_all=True, exceed_percent=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_mod_cell, pre_not_mod_cell = find_mod_cell(pre_HSE_peaks, pre_zscore, title='Pre ', win_size=300, \n",
    "                                               plot_aver=True, plot_all=False, exceed_percent=0.5)\n",
    "post_mod_cell, post_not_mod_cell = find_mod_cell(post_HSE_peaks, post_zscore, title='Post ', win_size=300, \n",
    "                                                 plot_aver=True, plot_all=False, exceed_percent=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mod_cell_num([len(pre_mod_cell), len(pre_not_mod_cell)], [len(post_mod_cell), len(post_not_mod_cell)])\n",
    "# pre+, pre-, post+, post-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b994842",
   "metadata": {},
   "outputs": [],
   "source": [
    "for peak in pre_HSE_peaks:\n",
    "    get_reactivated_cell(peak, pre_zscore, 30, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c48d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICA_weight = EB_assembly_analysis(Spks_behav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ICA_weight.shape[1]):\n",
    "    assembly_activation_strength(ICA_weight, list_pre[-1], list_post[-1], hse_win=30, plot_num=i) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis]",
   "language": "python",
   "name": "conda-env-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
